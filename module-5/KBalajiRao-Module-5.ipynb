{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123d327f",
   "metadata": {},
   "source": [
    "# Module 5 Assignment: Topic Modeling on COVID-19 Research Papers\n",
    "\n",
    "This assignment focuses on building and interpreting topic models using a dataset of research papers related to COVID-19, published before the 2020 pandemic. You will preprocess the text, vectorize it, build topic models, evaluate them, and interpret the topics.\n",
    "\n",
    "**Assignment Steps:**\n",
    "1. Pre-process the data (remove stop words, apply stemming/lemmatization)\n",
    "2. Vectorize the text using `CountVectorizer` (optionally with bigrams/trigrams)\n",
    "3. Build topic models with 8, 9, and 10 topics\n",
    "4. Evaluate models using perplexity and/or topic coherence\n",
    "5. Select and interpret a model, describe topics, and visualize results\n",
    "6. Summarize your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f618d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c1cb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "  Downloading numpy-2.3.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/24.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "\u001b[2K  Attempting uninstall: wrapt\n",
      "\u001b[2K    Found existing installation: wrapt 1.17.2\n",
      "\u001b[2K    Uninstalling wrapt-1.17.2:\n",
      "\u001b[2K      Successfully uninstalled wrapt-1.17.2\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "\u001b[2K  Attempting uninstall: wrapt\n",
      "\u001b[2K    Found existing installation: wrapt 1.17.2\n",
      "\u001b[2K    Uninstalling wrapt-1.17.2:\n",
      "\u001b[2K      Successfully uninstalled wrapt-1.17.2\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[2K  Attempting uninstall: smart-openm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: smart-open 7.1.0━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling smart-open-7.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled smart-open-7.1.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: scipy0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.13.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: smart-open━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: smart-open 7.1.0━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling smart-open-7.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled smart-open-7.1.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: scipy0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.13.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.13.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.13.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.13.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.13.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: gensim━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: gensim 4.3.30m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling gensim-4.3.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled gensim-4.3.3[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: gensim━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [gensim]\n",
      "\u001b[2K    Found existing installation: gensim 4.3.30m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling gensim-4.3.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled gensim-4.3.3[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [gensim]2m4/5\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "# Reinstall numpy, scipy, and gensim to fix binary incompatibility\n",
    "!pip install --force-reinstall --no-cache-dir numpy scipy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d33a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "  Downloading numpy-2.3.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting numpy\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/24.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mCollecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/14.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "\u001b[2K  Attempting uninstall: wrapt\n",
      "\u001b[2K    Found existing installation: wrapt 1.17.2\n",
      "\u001b[2K    Uninstalling wrapt-1.17.2:\n",
      "\u001b[2K      Successfully uninstalled wrapt-1.17.2\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "\u001b[2K  Attempting uninstall: wrapt\n",
      "\u001b[2K    Found existing installation: wrapt 1.17.2\n",
      "\u001b[2K    Uninstalling wrapt-1.17.2:\n",
      "\u001b[2K      Successfully uninstalled wrapt-1.17.2\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[2K  Attempting uninstall: smart-openm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: smart-open 7.1.0━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling smart-open-7.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled smart-open-7.1.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: smart-openm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: smart-open 7.1.0━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling smart-open-7.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled smart-open-7.1.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: scipy0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.13.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.13.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.13.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: scipy0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.13.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.13.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.13.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: gensim━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: gensim 4.3.30m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling gensim-4.3.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled gensim-4.3.3[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: gensim━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: gensim 4.3.30m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling gensim-4.3.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled gensim-4.3.3[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [gensim]2m4/5\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "# Remove all __pycache__ folders and force reinstall numpy and gensim\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "for root, dirs, files in os.walk(\".\", topdown=False):\n",
    "    for name in dirs:\n",
    "        if name == \"__pycache__\":\n",
    "            shutil.rmtree(os.path.join(root, name))\n",
    "\n",
    "# Now force reinstall numpy and gensim\n",
    "!pip install --force-reinstall --no-cache-dir numpy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1180d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af1ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (using a subset for demonstration if needed)\n",
    "data_dir = \"./module-5/dataset/biorxiv_medrxiv/biorxiv_medrxiv/\"\n",
    "file_list = glob.glob(os.path.join(data_dir, '*.json'))\n",
    "\n",
    "# Read a subset of files for faster processing (adjust n_files as needed)\n",
    "n_files = 100  # You can increase this if your machine can handle more\n",
    "papers = []\n",
    "for file in file_list[:n_files]:\n",
    "    with open(file, 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers.append(paper)\n",
    "\n",
    "# Extract text (e.g., title + abstract)\n",
    "def extract_text(paper):\n",
    "    title = paper.get('title', '')\n",
    "    abstract = paper.get('abstract', '')\n",
    "    if isinstance(abstract, list):\n",
    "        abstract = ' '.join([a.get('text', '') for a in abstract])\n",
    "    return f\"{title} {abstract}\"\n",
    "\n",
    "documents = [extract_text(p) for p in papers]\n",
    "\n",
    "# Preview a few documents\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"Document {i+1}:\\n\", doc[:500], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdd579",
   "metadata": {},
   "source": [
    "## Data Loading: All Subfolders\n",
    "We will load data from all four JSON subfolders using the provided `load_data` helper function. This ensures our topic model covers the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9ae2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute path: /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/biorxiv_medrxiv/biorxiv_medrxiv/\n",
      "Found 885 JSON files in /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/biorxiv_medrxiv/biorxiv_medrxiv/\n",
      "Loaded 5 documents from /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/biorxiv_medrxiv/biorxiv_medrxiv/\n",
      "Absolute path: /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/comm_use_subset/comm_use_subset/\n",
      "Found 9118 JSON files in /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/comm_use_subset/comm_use_subset/\n",
      "Loaded 5 documents from /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/comm_use_subset/comm_use_subset/\n",
      "Absolute path: /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/custom_license/custom_license/\n",
      "Found 16959 JSON files in /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/custom_license/custom_license/\n",
      "Loaded 5 documents from /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/custom_license/custom_license/\n",
      "Absolute path: /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/noncomm_use_subset/noncomm_use_subset/\n",
      "Found 2353 JSON files in /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/noncomm_use_subset/noncomm_use_subset/\n",
      "Loaded 5 documents from /Users/balaji/source/san-diego/assignments/Machine-learning-Fundamentals-and-Applications/module-5/dataset/noncomm_use_subset/noncomm_use_subset/\n",
      "Total loaded documents: 20\n",
      "Sample document:\n",
      " Multimerization of HIV-1 integrase hinges on conserved SH3-docking platforms New anti-AIDS treatments must be continually developed in order to overcome resistance mutations including those emerging in the newest therapeutic target, the viral integrase (IN). Multimerization of IN is functionally imperative and provides a forthcoming therapeutic target. Allosteric inhibitors of IN bind to non-catalytic sites and prevent correct multimerization not only restricting viral integration but also the a\n"
     ]
    }
   ],
   "source": [
    "# Load data from all four subfolders using the helper function\n",
    "# This will return a list of documents (strings)\n",
    "\n",
    "from helper_functions import load_data\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Use absolute paths relative to the notebook location\n",
    "base_path = os.path.abspath('dataset/')\n",
    "data_folders = [\n",
    "    os.path.join(base_path, 'biorxiv_medrxiv/biorxiv_medrxiv/'),\n",
    "    os.path.join(base_path, 'comm_use_subset/comm_use_subset/'),\n",
    "    os.path.join(base_path, 'custom_license/custom_license/'),\n",
    "    os.path.join(base_path, 'noncomm_use_subset/noncomm_use_subset/')\n",
    "]\n",
    "\n",
    "all_documents = []\n",
    "for folder in data_folders:\n",
    "    files = glob.glob(os.path.join(folder, '*.json'))\n",
    "    print(f\"Absolute path: {folder}\")\n",
    "    print(f\"Found {len(files)} JSON files in {folder}\")\n",
    "    if len(files) > 0:\n",
    "        docs = load_data(folder, n_files=5)  # Try with 5 for quick test\n",
    "        print(f\"Loaded {len(docs)} documents from {folder}\")\n",
    "        all_documents.extend(docs)\n",
    "    else:\n",
    "        print(f\"No JSON files found in {folder}. Please check the path and data.\")\n",
    "\n",
    "print(f\"Total loaded documents: {len(all_documents)}\")\n",
    "if all_documents:\n",
    "    print(\"Sample document:\\n\", all_documents[0][:500])\n",
    "else:\n",
    "    print(\"No documents loaded. Please check your folder paths and data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31bc91f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "  cov sars samples 2020 viral\n",
      "\n",
      "Topic 2:\n",
      "  cases figure site et al\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test helper functions to ensure they work\n",
    "# We'll use a small sample for demonstration\n",
    "sample_docs = all_documents[:5]\n",
    "\n",
    "# Display topics (dummy LDA for test)\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "X = vectorizer.fit_transform(sample_docs)\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Import display_topics from helper_functions and use it\n",
    "from helper_functions import display_topics\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ea110",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Stop Words Removal and Lemmatization\n",
    "We will preprocess the documents by removing stop words and applying lemmatization. This step helps to normalize the text and improve topic modeling quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33250417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multimer hiv integras hing conserv dock platform new anti aid treatment must continu develop order overcom resist mutat includ emerg newest therapeut target viral integras multimer function imper provid forthcom therapeut target alloster inhibitor bind non catalyt site prevent correct multimer restrict viral integr also assembl matur viral particl report alloster inhibitor peptid target unexploit dock platform retrovir crystal structur peptid complex hiv core domain reveal steric interfer would \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Prepare stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Use stemming instead of lemmatization to avoid wordnet dependency\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize using regex (splits on word boundaries)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Remove stop words and stem\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to all documents\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in all_documents]\n",
    "\n",
    "# Preview a preprocessed document\n",
    "print(preprocessed_documents[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e9892",
   "metadata": {},
   "source": [
    "## Vectorization: CountVectorizer with Bigrams/Trigrams Option\n",
    "We will vectorize the preprocessed text using CountVectorizer. You can adjust ngram_range for unigrams, bigrams, or trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc548d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix shape: (20, 3538)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# You can adjust ngram_range to (1,2) for bigrams or (1,3) for trigrams\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "print(f\"Document-term matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf5b48",
   "metadata": {},
   "source": [
    "## Topic Modeling: Train LDA Models for 8, 9, and 10 Topics\n",
    "We will train LDA models for different topic counts and evaluate them using perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5938cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA with 8 topics: Perplexity = 1160.01\n",
      "LDA with 9 topics: Perplexity = 1137.09\n",
      "LDA with 10 topics: Perplexity = 1151.82\n",
      "LDA with 10 topics: Perplexity = 1151.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics_list = [8, 9, 10]\n",
    "lda_models = {}\n",
    "perplexities = {}\n",
    "\n",
    "for n_topics in n_topics_list:\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method='batch')\n",
    "    lda.fit(X)\n",
    "    lda_models[n_topics] = lda\n",
    "    perplexity = lda.perplexity(X)\n",
    "    perplexities[n_topics] = perplexity\n",
    "    print(f\"LDA with {n_topics} topics: Perplexity = {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6660e",
   "metadata": {},
   "source": [
    "## Topic Model Evaluation and Selection\n",
    "We compare perplexity scores to select the best topic count. Lower perplexity indicates a better model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95e3435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 8, Perplexity: 1160.01\n",
      "Topics: 9, Perplexity: 1137.09\n",
      "Topics: 10, Perplexity: 1151.82\n",
      "\n",
      "Selected 9 topics as the best model.\n"
     ]
    }
   ],
   "source": [
    "# Display perplexity scores for each model\n",
    "for n_topics, perp in perplexities.items():\n",
    "    print(f\"Topics: {n_topics}, Perplexity: {perp:.2f}\")\n",
    "\n",
    "# Select the best model (lowest perplexity)\n",
    "best_n_topics = min(perplexities, key=perplexities.get)\n",
    "best_lda = lda_models[best_n_topics]\n",
    "print(f\"\\nSelected {best_n_topics} topics as the best model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2fda6",
   "metadata": {},
   "source": [
    "## Topic Interpretation: Top Words and Descriptions\n",
    "We extract the top words for each topic and provide a brief interpretation for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdcf3854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "  al et al et cell virus viral estim activ sampl use\n",
      "\n",
      "Topic 2:\n",
      "  cov season sar sampl sar cov patient viru viral peak transmiss\n",
      "\n",
      "Topic 3:\n",
      "  hong hong kong kong polic particip plasma day studi care influenza\n",
      "\n",
      "Topic 4:\n",
      "  intern co per per cent cent intens growth increas global demand\n",
      "\n",
      "Topic 5:\n",
      "  protein express gene immun optim codon vaccin challeng protect day\n",
      "\n",
      "Topic 6:\n",
      "  cell protein viral dna rna mrna ribosom viru virus translat\n",
      "\n",
      "Topic 7:\n",
      "  case preprint doi author number estim time peer peer review review\n",
      "\n",
      "Topic 8:\n",
      "  cell protein neutral viru antibodi infect use ml express incub\n",
      "\n",
      "Topic 9:\n",
      "  cell ifn viru infect candid strain murin human mice ml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import display_topics\n",
    "\n",
    "# Display top words for each topic in the best model\n",
    "display_topics(best_lda, vectorizer.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e6e40",
   "metadata": {},
   "source": [
    "### Topic Descriptions\n",
    "Write 1–2 sentences describing the main theme of each topic based on the top words above. What makes each topic stand out?\n",
    "\n",
    "1. **Topic 1:** This topic focuses on general virology and laboratory methods, with frequent mentions of 'cell', 'virus', 'viral', and 'sample'. It likely covers experimental studies and viral activity measurements.\n",
    "2. **Topic 2:** This topic is centered on coronaviruses, especially SARS and seasonal coronaviruses, with terms like 'cov', 'sar', 'patient', and 'transmiss'. It likely discusses patient data and transmission dynamics.\n",
    "3. **Topic 3:** This topic appears to relate to studies in Hong Kong, with references to 'hong kong', 'polic', 'particip', and 'care', possibly covering epidemiological studies or public health responses in that region.\n",
    "4. **Topic 4:** This topic is about international and global trends, with words like 'intern', 'per cent', 'growth', and 'global demand', suggesting a focus on global health, resource needs, or epidemiological trends.\n",
    "5. **Topic 5:** This topic is focused on immunology and vaccine development, with terms like 'protein', 'express', 'gene', 'vaccin', and 'protect', indicating studies on immune response and vaccine efficacy.\n",
    "6. **Topic 6:** This topic covers molecular biology, especially genetic material and translation, with words like 'cell', 'protein', 'dna', 'rna', 'mrna', and 'translat', likely discussing viral replication and gene expression.\n",
    "7. **Topic 7:** This topic is about publication and peer review, with terms like 'case', 'preprint', 'doi', 'author', 'peer review', and 'time', indicating meta-scientific or bibliometric analysis.\n",
    "8. **Topic 8:** This topic focuses on neutralization and antibody studies, with words like 'cell', 'protein', 'neutral', 'antibodi', 'infect', and 'incub', likely covering laboratory assays and immune response.\n",
    "9. **Topic 9:** This topic is about animal models and infection studies, with terms like 'cell', 'ifn', 'infect', 'strain', 'murin', 'mice', and 'human', suggesting research on infection mechanisms in animal models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327099c1",
   "metadata": {},
   "source": [
    "### Overall Summary\n",
    "Summarize all topics in 1–2 sentences, highlighting the diversity or focus of the research papers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
